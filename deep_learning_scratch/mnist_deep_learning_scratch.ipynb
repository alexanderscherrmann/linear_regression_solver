{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unified_shuffle(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = unified_shuffle(digits['data'], digits['target'])\n",
    "split = int(y.size * 0.8)\n",
    "X_train, y_train, X_val, y_val = X[:split].T/255, y[:split], X[split:].T/255, y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(X,y,n_neurons):\n",
    "    n_classes = np.unique(y).size\n",
    "    W1 = np.random.randn(n_neurons, X.shape[0]) * 0.01\n",
    "    b1 = np.random.randn(n_neurons,1)\n",
    "    W2 = np.random.randn(n_classes,n_neurons) * 0.01\n",
    "    b2 = np.random.randn(n_classes,1)\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def soft_max(Z):\n",
    "    exps = np.exp(Z - np.max(Z, axis=0, keepdims=True))  # Normalize to prevent overflow\n",
    "    return exps / np.sum(exps, axis=0, keepdims=True) \n",
    "\n",
    "def forward(W1,b1,W2,b2,X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLu(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = soft_max(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def deriv_ReLU(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_y = np.zeros((Y.size,np.unique(Y).size))\n",
    "    ## loop trhough each entry and then set the corresponding value to 1\n",
    "    one_hot_y[np.arange(Y.size),Y] = 1\n",
    "    return one_hot_y.T\n",
    "\n",
    "def backward(Z1,A1,A2,W2,X,y):\n",
    "    m = y.size\n",
    "    one_hot_y = one_hot(y)\n",
    "    dZ2 = A2 - one_hot_y\n",
    "    dW2 = 1/m * dZ2.dot(A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2,1,keepdims=True)\n",
    "    dZ1 = W2.T.dot(dZ2) * deriv_ReLU(Z1)\n",
    "    dW1 = 1/m * dZ1.dot(X.T)\n",
    "    db1 = 1/m * np.sum(dZ1,1,keepdims=True)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(A2):\n",
    "    return np.argmax(A2,0)\n",
    "\n",
    "def get_acc(predictions,y_true):\n",
    "    return np.sum(predictions == y_true)/y_true.size\n",
    "\n",
    "def grad_descent(X,y,iterations,alpha,n_neurons):\n",
    "    W1, b1, W2, b2 = init_params(X,y,n_neurons)\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward(W1,b1,W2,b2,X)\n",
    "        dW1, db1, dW2, db2 = backward(Z1,A1,A2,W2,X,y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 50 == 0:\n",
    "            print(\"Iteration\", i)\n",
    "            print(\"Current Accuracy:\", get_acc(get_prediction(A2),y))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Current Accuracy: 0.09035\n",
      "Iteration 50\n",
      "Current Accuracy: 0.6057666666666667\n",
      "Iteration 100\n",
      "Current Accuracy: 0.7905166666666666\n",
      "Iteration 150\n",
      "Current Accuracy: 0.8454\n",
      "Iteration 200\n",
      "Current Accuracy: 0.8701166666666666\n",
      "Iteration 250\n",
      "Current Accuracy: 0.8819333333333333\n",
      "Iteration 300\n",
      "Current Accuracy: 0.8886833333333334\n",
      "Iteration 350\n",
      "Current Accuracy: 0.8944833333333333\n",
      "Iteration 400\n",
      "Current Accuracy: 0.8980666666666667\n",
      "Iteration 450\n",
      "Current Accuracy: 0.9015\n",
      "Iteration 500\n",
      "Current Accuracy: 0.9042166666666667\n",
      "Iteration 550\n",
      "Current Accuracy: 0.9064833333333333\n",
      "Iteration 600\n",
      "Current Accuracy: 0.9092333333333333\n",
      "Iteration 650\n",
      "Current Accuracy: 0.9114666666666666\n",
      "Iteration 700\n",
      "Current Accuracy: 0.91335\n",
      "Iteration 750\n",
      "Current Accuracy: 0.9148833333333334\n",
      "Iteration 800\n",
      "Current Accuracy: 0.9165166666666666\n",
      "Iteration 850\n",
      "Current Accuracy: 0.918\n",
      "Iteration 900\n",
      "Current Accuracy: 0.9195833333333333\n",
      "Iteration 950\n",
      "Current Accuracy: 0.92095\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = grad_descent(X_train.T, y_train, 1000, 0.1, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Mnist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from array import array\n",
    "from os.path import join\n",
    "\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "# Set file paths based on added MNIST Datasets\n",
    "#\n",
    "input_path = '/home/alexanderscherrmann/Coding/python/ml/scratch_projects/deep_learning_scratch/'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "#\n",
    "# Helper function to show a list of images with their relating titles\n",
    "#\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1    \n",
    "    for x in zip(images, title_texts):        \n",
    "        image = x[0]        \n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1\n",
    "\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (np.array(x_train).reshape(np.array(x_train).shape[0],-1)/255, np.array(y_train)),(np.array(x_test).reshape(np.array(x_test).shape[0],-1)/255, np.array(y_test))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(X_train, y_train), (X_val, y_val) = mnist_dataloader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Current Accuracy: 0.08956666666666667\n",
      "Iteration 50\n",
      "Current Accuracy: 0.14415\n",
      "Iteration 100\n",
      "Current Accuracy: 0.18565\n",
      "Iteration 150\n",
      "Current Accuracy: 0.22513333333333332\n",
      "Iteration 200\n",
      "Current Accuracy: 0.2742\n",
      "Iteration 250\n",
      "Current Accuracy: 0.29991666666666666\n",
      "Iteration 300\n",
      "Current Accuracy: 0.3221833333333333\n",
      "Iteration 350\n",
      "Current Accuracy: 0.34313333333333335\n",
      "Iteration 400\n",
      "Current Accuracy: 0.36336666666666667\n",
      "Iteration 450\n",
      "Current Accuracy: 0.38311666666666666\n",
      "Iteration 500\n",
      "Current Accuracy: 0.42766666666666664\n",
      "Iteration 550\n",
      "Current Accuracy: 0.4428\n",
      "Iteration 600\n",
      "Current Accuracy: 0.4577\n",
      "Iteration 650\n",
      "Current Accuracy: 0.4714333333333333\n",
      "Iteration 700\n",
      "Current Accuracy: 0.4842\n",
      "Iteration 750\n",
      "Current Accuracy: 0.49793333333333334\n",
      "Iteration 800\n",
      "Current Accuracy: 0.5105333333333333\n",
      "Iteration 850\n",
      "Current Accuracy: 0.5220333333333333\n",
      "Iteration 900\n",
      "Current Accuracy: 0.53415\n",
      "Iteration 950\n",
      "Current Accuracy: 0.5456666666666666\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = grad_descent(X_train.T, y_train, 1000, 0.1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        # Initialize with He initialization for better gradient flow\n",
    "        self.params = {\n",
    "            'W1': np.random.randn(hidden_size, input_size) * np.sqrt(2. / input_size),\n",
    "            'b1': np.zeros((hidden_size, 1)),\n",
    "            'W2': np.random.randn(output_size, hidden_size) * np.sqrt(2. / hidden_size),\n",
    "            'b2': np.zeros((output_size, 1))\n",
    "        }\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def relu(self, Z):\n",
    "        \"\"\"ReLU activation function\"\"\"\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def relu_derivative(self, Z):\n",
    "        \"\"\"Derivative of ReLU function\"\"\"\n",
    "        return Z > 0\n",
    "    \n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax activation function with numerical stability\"\"\"\n",
    "        exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "        return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    \n",
    "    def one_hot_encode(self, y, num_classes=None):\n",
    "        \"\"\"Convert label vector to one-hot encoded matrix\"\"\"\n",
    "        if num_classes is None:\n",
    "            num_classes = np.max(y) + 1\n",
    "        one_hot = np.zeros((y.size, num_classes))\n",
    "        one_hot[np.arange(y.size), y] = 1\n",
    "        return one_hot.T\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # Store intermediate values for backpropagation\n",
    "        self.cache = {}\n",
    "        \n",
    "        # First layer\n",
    "        self.cache['X'] = X\n",
    "        self.cache['Z1'] = np.dot(self.params['W1'], X) + self.params['b1']\n",
    "        self.cache['A1'] = self.relu(self.cache['Z1'])\n",
    "        \n",
    "        # Output layer\n",
    "        self.cache['Z2'] = np.dot(self.params['W2'], self.cache['A1']) + self.params['b2']\n",
    "        self.cache['A2'] = self.softmax(self.cache['Z2'])\n",
    "        \n",
    "        return self.cache['A2']\n",
    "    \n",
    "    def backward(self, y, output_activation=None):\n",
    "        \"\"\"Backward pass to compute gradients\"\"\"\n",
    "        if output_activation is None:\n",
    "            output_activation = self.cache['A2']\n",
    "            \n",
    "        m = y.shape[0]  # Number of examples\n",
    "        \n",
    "        # Convert labels to one-hot encoding if needed\n",
    "        if len(y.shape) == 1:\n",
    "            y_one_hot = self.one_hot_encode(y)\n",
    "        else:\n",
    "            y_one_hot = y.T  # Assuming y is already one-hot encoded but needs transposition\n",
    "            \n",
    "        # Output layer gradients\n",
    "        dZ2 = output_activation - y_one_hot\n",
    "        dW2 = (1 / m) * np.dot(dZ2, self.cache['A1'].T)\n",
    "        db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dZ1 = np.dot(self.params['W2'].T, dZ2) * self.relu_derivative(self.cache['Z1'])\n",
    "        dW1 = (1 / m) * np.dot(dZ1, self.cache['X'].T)\n",
    "        db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        \n",
    "        # Store gradients\n",
    "        self.gradients = {\n",
    "            'W1': dW1, 'b1': db1,\n",
    "            'W2': dW2, 'b2': db2\n",
    "        }\n",
    "        \n",
    "        return self.gradients\n",
    "    \n",
    "    def update_parameters(self, gradients=None):\n",
    "        \"\"\"Update parameters using gradient descent\"\"\"\n",
    "        if gradients is None:\n",
    "            gradients = self.gradients\n",
    "            \n",
    "        for key in self.params:\n",
    "            self.params[key] -= self.learning_rate * gradients[key]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions using the trained model\"\"\"\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=0)\n",
    "    \n",
    "    def calculate_accuracy(self, X, y):\n",
    "        \"\"\"Calculate accuracy of predictions\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "    \n",
    "    def calculate_loss(self, X, y):\n",
    "        \"\"\"Calculate cross-entropy loss\"\"\"\n",
    "        m = y.shape[0]\n",
    "        output = self.forward(X)\n",
    "        \n",
    "        # Convert y to one-hot if needed\n",
    "        if len(y.shape) == 1:\n",
    "            y_one_hot = self.one_hot_encode(y)\n",
    "        else:\n",
    "            y_one_hot = y.T\n",
    "        \n",
    "        # Avoid log(0) by adding a small epsilon\n",
    "        epsilon = 1e-15\n",
    "        log_probs = -np.log(output + epsilon) * y_one_hot\n",
    "        loss = np.sum(log_probs) / m\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, print_every=100, X_val=None, y_val=None):\n",
    "        \"\"\"Train the network\"\"\"\n",
    "        # Ensure X is in the correct shape (features, samples)\n",
    "        if X.shape[0] < X.shape[1] and X.shape[0] == y.size:\n",
    "            X = X.T  # Transpose if samples are in rows instead of columns\n",
    "            \n",
    "        history = {'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []}\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            # Forward and backward passes\n",
    "            self.forward(X)\n",
    "            self.backward(y)\n",
    "            self.update_parameters()\n",
    "            \n",
    "            # Record metrics\n",
    "            if i % print_every == 0 or i == epochs - 1:\n",
    "                train_acc = self.calculate_accuracy(X, y)\n",
    "                train_loss = self.calculate_loss(X, y)\n",
    "                history['train_acc'].append(train_acc)\n",
    "                history['train_loss'].append(train_loss)\n",
    "                \n",
    "                print(f\"Epoch {i}, Train Accuracy: {train_acc:.4f}, Loss: {train_loss:.4f}\")\n",
    "                \n",
    "                # Validation if provided\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    val_acc = self.calculate_accuracy(X_val, y_val)\n",
    "                    val_loss = self.calculate_loss(X_val, y_val)\n",
    "                    history['val_acc'].append(val_acc)\n",
    "                    history['val_loss'].append(val_loss)\n",
    "                    print(f\"Validation Accuracy: {val_acc:.4f}, Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        return history\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def prepare_data(X, y):\n",
    "    \"\"\"Prepare data for the neural network\"\"\"\n",
    "    # Convert to numpy arrays if they aren't already\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Ensure X is in shape (features, samples)\n",
    "    if X.shape[0] == y.size:\n",
    "        X = X.T\n",
    "    \n",
    "    # Normalize features\n",
    "    X = (X - np.mean(X, axis=1, keepdims=True)) / np.std(X, axis=1, keepdims=True)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    \"\"\"Split data into training and test sets\"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    m = y.size\n",
    "    indices = np.random.permutation(m)\n",
    "    test_size = int(m * test_size)\n",
    "    \n",
    "    test_idx = indices[:test_size]\n",
    "    train_idx = indices[test_size:]\n",
    "    \n",
    "    X_train = X[:, train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_test = X[:, test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_tra).reshape(np.array(X_tra).shape[0],-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12529/3969906844.py:162: RuntimeWarning: invalid value encountered in divide\n",
      "  X = (X - np.mean(X, axis=1, keepdims=True)) / np.std(X, axis=1, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Accuracy: 0.0989, Loss: nan\n",
      "Validation Accuracy: 0.0979, Loss: nan\n",
      "Epoch 50, Train Accuracy: 0.0989, Loss: nan\n",
      "Validation Accuracy: 0.0979, Loss: nan\n",
      "Epoch 100, Train Accuracy: 0.0989, Loss: nan\n",
      "Validation Accuracy: 0.0979, Loss: nan\n",
      "Epoch 150, Train Accuracy: 0.0989, Loss: nan\n",
      "Validation Accuracy: 0.0979, Loss: nan\n",
      "Epoch 200, Train Accuracy: 0.0989, Loss: nan\n",
      "Validation Accuracy: 0.0979, Loss: nan\n",
      "Epoch 250, Train Accuracy: 0.0989, Loss: nan\n",
      "Validation Accuracy: 0.0979, Loss: nan\n",
      "Epoch 300, Train Accuracy: 0.0989, Loss: nan\n",
      "Validation Accuracy: 0.0979, Loss: nan\n",
      "Epoch 350, Train Accuracy: 0.0989, Loss: nan\n",
      "Validation Accuracy: 0.0979, Loss: nan\n",
      "Epoch 400, Train Accuracy: 0.0989, Loss: nan\n",
      "Validation Accuracy: 0.0979, Loss: nan\n",
      "Epoch 450, Train Accuracy: 0.0989, Loss: nan\n",
      "Validation Accuracy: 0.0979, Loss: nan\n",
      "Epoch 499, Train Accuracy: 0.0989, Loss: nan\n",
      "Validation Accuracy: 0.0979, Loss: nan\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "X_tr = np.array(X_tra).reshape(np.array(X_tra).shape[0],-1)\n",
    "X_data, y_data = prepare_data(X_tr, y_tra)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "model = NeuralNetwork(input_size=X_train.shape[0], hidden_size=64, output_size=len(np.unique(y_train)))\n",
    "history = model.train(X_train, y_train, epochs=500, print_every=50, X_val=X_test, y_val=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
